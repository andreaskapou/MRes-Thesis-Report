\subsection{Extending BCC model for NGS data}\label{integr-extension-subsect}
With the advent of high-throughput sequencing methods, we need to extend the BCC method so it can model data that follow different probability distributions.

Due to conditional independence structure of the mixture models, changing the observation model for data source $\mathbb{X}_{m}$ does not affect the mixing proportions $\pi$, the adherence parameter $\alpha$, and the overall clustering $\mathbb{C}$. Hence, modelling and subsequently inferring these variables during Gibbs sampling, remains the same as it was explained in the previous section.

The source-specific clustering assignments $\mathbb{L}_{m}$ depend on the observation model of that data source $\mathbb{X}_{m}$ follows. Hopefully, the update equations for $\mathbb{L}_{m}$ on each Gibbs simulation step are straightforward to adapt, since we just need to evaluate $f_{m}(X_{mn}|\theta_{m})$ pointwise, which for most known probability distribution is easy to compute.

Thus, the focus of this section is mainly in modelling, \ie defining a prior distribution, and inferring the parameters $\theta_{m}$ of the observation model. 

\subsubsection*{Gaussian observation model}
For completeness we provide details for the Gaussian observation model, as it was described in \citet{Lock2013}. When the data source $\mathbb{X}_{m}$ follows a Gaussian mixture distribution, we have:
\begin{equation}
	X_{mn} \mid L_{mn} = k, \theta_{mk} \sim \mathcal{N}\big(\mu_{mk}, \tau_{mk}^{-1}\big)
\end{equation}
where $\mathcal{N}$ denotes the \emph{Normal} or \emph{Gaussian} distribution, $\mu_{mk}$ is the \emph{mean} parameter, and $\tau_{mk}$ is the \emph{precision} parameter, \ie the reciprocal of the variance, $\tau_{mk} = \frac{1}{\sigma_{mk}^{2}}$.

We choose a \emph{Normal-Gamma} conjugate prior distribution for the parameters $\theta_{mk} = (\mu_{mk}, \tau_{mk})$, that is:
\begin{equation}
	\theta_{mk} \sim \mathcal{NG}amma\big(\mu_{m0}, \tau_{m0}, \mathit{a}_{m0}, \beta_{m0}\big)
\end{equation}
where hyper-parameters $\mu_{m0}, \tau_{m0}$ are the same as defined above, and $\mathit{a}_{m0}, \beta_{m0}$ are the \emph{shape} and \emph{rate} hyper-parameters of the \emph{Gamma} distribution, respectively.

Thus, on each iteration step of Gibbs sampling algorithm, we have the following updates for each parameter:
\begin{equation}
  \begin{aligned}
  	\tau_{mk} \mid \mathbb{L}_{m}=k, \mathbb{X}_{m} \;& \sim \;\mathcal{G}amma\big(\hat{\mathit{a}}_{m0}, \hat{\beta}_{m0}\big) \\
	\mu_{mk} \mid \tau_{mk}, \mathbb{L}_{m}=k, \mathbb{X}_{m} \; & \sim \; \mathcal{N}\big(\hat{\mu}_{m0}, (\hat{\tau}_{m0} \tau_{mk})^{-1}\big)
  \end{aligned}
\end{equation}
The posterior parameter values for the $\mathcal{NG}(\cdot,\cdot,\cdot,\cdot)$ distribution are calculated as follows:
\begin{equation}
  \begin{aligned}
  	\hat{\mu}_{m0} \; &= \; \frac{\tau_{m0}\mu_{m0} + N_{mk}\mathcal{X}_{mk}}{\tau_{m0} + N_{mk}}\\
  	\hat{\tau}_{m0} \; &= \; \tau_{m0} + N_{mk}\\
  	\hat{\mathit{a}}_{m0} \; &= \; \mathit{a}_{m0} + \frac{N_{mk}}{2}\\
  	\hat{\beta}_{m0} \; &= \; \beta_{m0} + \frac{SSD_{mk}}{2} + \frac{N_{mk}\tau_{m0}(\mathcal{X}_{mk} - \mu_{m0})}{2(N_{mk}+\tau_{m0})}
  \end{aligned}
\end{equation}
where: 
\begin{equation}
  \begin{aligned}
		N_{mk} \; &= \; \sum_{n}\mathbbm{1}(L_{mn}=k)\\ 
		\mathcal{X}_{mk} \; &= \; \frac{\sum_{n}\mathbbm{1}(L_{mn}=k)X_{mn}}{N_{mk}} \\
		SSD_{mk} \; &= \; \sum_{n}\mathbbm{1}(L_{mn}=k)(X_{mn}-\mathcal{X}_{mk})^{2}
  \end{aligned}
\end{equation} 

If the data source $\mathbb{X}_{m}$ is $D_{m}$-dimensional, then we can use either a $D_{m}$ dimensional \emph{Normal-Gamma} prior, if we assume a $D_{m} \times D_{m}$ \emph{diagonal precision} matrix, or a \emph{Normal-Wishart} prior, if we assume a $D_{m} \times D_{m}$ \emph{full precision} matrix \cite[Ch. 2]{Bishop2006}.

\subsubsection*{Binomial observation model}
When the data source $\mathbb{X}_{m}$ follows a Binomial mixture distribution, we have:
\begin{equation}
	X_{mn} \mid L_{mn} = k, \theta_{mk} \sim \mathcal{B}inom\big(t_{mk}, \rho_{mk}\big)
\end{equation}
where $t_{mk}$ is \emph{known} and denotes the total number of experiments, and $\rho_{mk}$ is the parameter for the probability of success.

We choose a \emph{Beta} conjugate prior distribution for the parameter $\rho_{mk}$, that is:
\begin{equation}
	\rho_{mk} \sim \mathcal{B}eta\big(\mathit{a}_{m0}, \beta_{m0}\big)
\end{equation}
where $\mathit{a}_{m0}$ and $\beta_{m0}$ are the \emph{shape} hyper-parameters of the \emph{Beta} distribution.

Thus, on each iteration step of Gibbs sampling algorithm, we have the following update for the $\rho_{mk}$ parameter:
\begin{equation}
  \begin{aligned}
  	\rho_{mk} \mid \mathbb{L}_{m}=k, \mathbb{X}_{m} \;& \sim \;\mathcal{B}eta\big(\hat{\mathit{a}}_{m0}, \hat{\beta}_{m0}\big) \\
  \end{aligned}
\end{equation}
where the posterior parameter values for the $\mathcal{B}eta(\cdot,\cdot)$ distribution are calculated as follows:
\begin{equation}
  \begin{aligned}
  	\hat{\mathit{a}}_{m0} \; &= \; \mathit{a}_{m0} + \sum_{n}\mathbbm{1}(L_{mn}=k)X_{mn} \\
  	\hat{\beta}_{m0} \; &= \; \beta_{m0} +  \sum_{n}\mathbbm{1}(L_{mn}=k)(r_{mn} - X_{mn})
  \end{aligned}
\end{equation}

\subsubsection*{Poisson observation model}
When the data source $\mathbb{X}_{m}$ follows a Poisson mixture distribution, we have:
\begin{equation}
	X_{mn} \mid L_{mn} = k, \theta_{mk} \sim \mathcal{P}ois\big(\lambda_{mk}\big)
\end{equation}
where $\lambda_{mk}$ denotes the mean and variance parameter.

We choose a \emph{Gamma} conjugate prior distribution for the parameter $\lambda_{mk}$, that is:
\begin{equation}
	\lambda_{mk} \sim \mathcal{G}amma\big(\mathit{a}_{m0}, \beta_{m0}\big)
\end{equation}
where $\mathit{a}_{m0}$ and $\beta_{m0}$ are the \emph{shape} and \emph{rate} hyper-parameters of the \emph{Gamma} distribution.

Thus, on each iteration step of Gibbs sampling algorithm, we have the following update for the $\lambda_{mk}$ parameter:
\begin{equation}
  \begin{aligned}
  	\lambda_{mk} \mid \mathbb{L}_{m}=k, \mathbb{X}_{m} \;& \sim \;\mathcal{G}amma\big(\hat{\mathit{a}}_{m0}, \hat{\beta}_{m0}\big) \\
  \end{aligned}
\end{equation}
where the posterior parameter values for the $\mathcal{G}amma(\cdot,\cdot)$ distribution are calculated as follows:
\begin{equation}
  \begin{aligned}
  	\hat{\mathit{a}}_{m0} \; &= \; \mathit{a}_{m0} + \sum_{n}\mathbbm{1}(L_{mn}=k)X_{mn} \\
  	\hat{\beta}_{m0} \; &= \; \beta_{m0} +  \sum_{n}\mathbbm{1}(L_{mn}=k)
  \end{aligned}
\end{equation}
