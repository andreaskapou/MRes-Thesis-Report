\section{Future Work} \label{future-work-sect}
In this final section we suggest some directions for future work. The time constraint for implementing this MSc project did not allow us to perform deep analysis of the proposed methods on real datasets. Hence, this thesis was mainly concentrated on the modelling aspect by showing that the proposed models have the statistical power to learn the true processes that generated the data.

The choice of the ENCODE datasets probably was not optimal for performing data analysis. A better approach would be to choose targeted experiments that come from same cells on different conditions and have a meaningful biological question to be answered. This would require a close collaboration with experimental biologists, in order to produce these datasets and also perform comprehensive analysis on the output of the methods from a biological perspective. 

Recently, we established a collaboration with Dr. Peter Adams' lab at the Beatson Institute for Cancer Research \citep{Adams2015}. The Adams' lab is interested in investigating the impact of the chromatin structure and epigenomics on cell ageing and cancer. We expect to extend these models and propose more elaborate methods in the near future to analyse the data that will be generated from these experiments, and will help to gain a better insight on the impact of epigenomics on cancer and ageing.     

\subsection{Mixture modelling of methylation profiles in BCC model} \label{conc-meth-prof-bcc-subsect}
In \emph{Chapter \ref{model-meth-chapter}} a novel method was proposed to model and cluster methylation profiles using the \emph{Binomial distributed Probit regression} function. Experimental evaluation on synthetic and real datasets confirmed the statistical power of the model to capture important features of the data. Even though this model is promising in clustering methylation profiles there are practical difficulties in integrating it inside Bayesian Consensus Clustering. 

The model is fitted to the data using a maximum likelihood approach and the Generalised EM algorithm is used to estimate the model parameters (\ie the coefficients of the latent basis functions). To integrate this model in the BCC model we need to adopt a fully Bayesian approach and compute the posterior distribution over the parameters, instead of only maximizing the likelihood function.

Deriving Gibbs sampling for simpler models, \eg mixture of Gaussians, is straightforward since \emph{conjugate priors} are introduced for the model parameters, and thus we can write analytically and sample from the full conditional distributions. For the \emph{Binomial distributed Probit regression} model, proposing conjugate priors for the parameters is difficult, thus deriving analytically the update equations and sampling from the full conditional distribution of each parameter is infeasible. Another approach is to use Metropolis-Hastings (MH) algorithm (see \emph{Section \ref{approx-bayes-infer-subsect}}), which is more general and can be used for any inference task, with the price that would be quite slow to converge, making actually the algorithm impractical for real datasets. 

A different approach is to resort to deterministic approximation schemes, such as \emph{Variational Bayes} \citep{Beal2003}. This implies that the inference task for the BCC model needs to be modified and the update equations for each parameter need to be derived. Variational inference algorithms construct a lower bound on the marginal likelihood and attempt to optimise this bound using a iterative scheme similar to the EM algorithm. Essentially, the inference task reduces to an optimisation problem where we only need to derive the update equations for each observation model. Even though this approach may be more mathematically involved, the algorithm is expected to be faster and the label-switching problem \citep{Stephens2000} will not be an issue.


\subsection{Parametrized Poisson mixture model for NGS data}
\citet{Rau2013} proposed a different parametrization of the Poisson Mixture Model (PMM) to account for specific characteristics of RNA-Seq data, such as difference in library size and dependence of measured expression level on gene length. The model they proposed is fitted to the data using maximum likelihood and EM algorithm is used to estimate the mixture model parameters. To be able to integrate this model in the BCC model, we need to adopt a fully Bayesian approach and compute the posterior distribution over the parameters. Deriving Gibbs sampling for this model requires deriving an expression for the full conditional distribution of the unknown parameters. In \emph{Appendix \ref{app-poisson}} we introduce the parametrized Poisson mixture model and also provide the whole derivation for the parameter updates at each MCMC iteration.

The proposed model is already integrated inside BCC, but due to time constraint it could not be evaluated on real datasets. It would be interesting to compare this extended model with the simple PMM and understand the weaknesses and strengths of each model when they are applied to different datasets. In general, we expect the parametrized PMM to have superior performance, since it is targeted for NGS data and is more flexible in the sense that different normalization techniques for the library size can be be easily applied in the model, \eg \emph{Trimmed Mean of M-values (TMM)} \citep{Robinson2010} and \emph{DESeq scaling factors} \citep{Anders2010}. 

A useful future direction for performing gene co-expression analysis would be to use a Negative Binomial mixture model instead of a PMM, as recently used by \citet{Si2013}. The Negative Binomial distribution is more appropriate in modelling NGS data, due to the additional variation observed from biological replicates \citep{Robinson2007}.
