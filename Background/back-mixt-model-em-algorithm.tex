\subsection{The EM algorithm} \label{em-algorithm-l-subsect}
Expectation Maximization is a general iterative algorithm for computing MLEs when there are missing data or latent variables. As aforementioned, mixture models can be formulated as LVMs, thus, EM arises naturally and alternates between inferring the latent values given the parameters (\emph{E-step}), and then optimizing the parameters given the filled in data (\emph{M-step}). EM exploits the fact that if the data were fully observed then the MLE would be easy to compute. In their classic paper, \cite{Dempster1977} proved that EM monotonically increases the log likelihood and it converges to a maximum likelihood estimator. The algorithm ends when a convergence criterion is satisfied, \eg the log likelihood does not increase above a certain threshold between two consecutive iterations. \emph{Fig. \ref{em-algorithm-pic}} illustrates the EM algorithm for the mixture of two Gaussian distributions applied to the Old Faithful dataset.
\begin{figure}[ht!]
     \begin{center}
        \subfigure[]{
            \label{fig:first}
            \includegraphics[width=0.3\textwidth]{images/em-a.jpg}
        }
        \subfigure[]{
           \label{fig:second}
           \includegraphics[width=0.3\textwidth]{images/em-b.jpg}
        }
        \subfigure[]{
            \label{fig:third}
            \includegraphics[width=0.3\textwidth]{images/em-c.jpg}
        } %  ------- End of the first row ----------------------%
        \subfigure[]{
            \label{fig:fourth}
            \includegraphics[width=0.3\textwidth]{images/em-d.jpg}
        }
        \subfigure[]{
            \label{fig:fifth}
            \includegraphics[width=0.3\textwidth]{images/em-e.jpg}
        }
        \subfigure[]{
            \label{fig:sixth}
            \includegraphics[width=0.3\textwidth]{images/em-f.jpg}
        }
    \end{center}
    \caption{\emph{Illustration of the EM algorithm using the Old Faithful dataset. Plot (a) shows initial parameters. Plots (b) and (c) show the E and M steps in the first iteration, respectively. Plots (d), (e) and (f) show the results after 2, 5 and 20 complete iterations of EM algorithm, respectively. \cite[Ch. \ 9]{Bishop2006}.}}
   \label{em-algorithm-pic}
\end{figure}

Our presentation for the derivation of the EM algorithm is based on \cite[Ch. \ 11]{Murphy2012}. Let $x_{i}$ be the observed variables, and $z_{i}$ be the hidden or latent variables. We are interested in maximizing the log likelihood of the observed data:
\begin{equation} \label{log-lik-observed-f-mm}
	\ell(\Theta) \triangleq \sum_{i=1}^{N} \log p(x_{i}|\theta) =  \sum_{i=1}^{N} \log \bigg[\sum_{z_{i}} p(x_{i}, z_{i}|\theta) \bigg]
\end{equation}
which is hard to optimize due to the presence of the summation inside the logarithm, so the logarithm no longer acts directly on the likelihood function.

Let us define the \emph{complete data log likelihood} as follows:
\begin{equation} \label{log-lik-comp-observed-f-mm}
	\ell_{c}(\Theta) \triangleq \sum_{i=1}^{N} \log p(x_{i}, z_{i}|\theta)
\end{equation}

If the variables $z_{i}$ were observed, we assume that this likelihood could be easy to compute. EM gets around this problem by defining the \emph{expected complete data log likelihood} as:

\begin{equation} \label{log-lik-expected-f-mm}
		Q(\Theta, \Theta^{t-1}) \triangleq \mathbb{E} \big[\ell_{c}(\Theta) | \mathbf{X}, \Theta^{t-1}\big]
\end{equation}
where $t$ is the current iteration. The expectation is taken with respect to the old parameters $\Theta^{t-1}$ and the observed data $\mathbf{X}$. In the \emph{E-step}, we compute the terms inside $Q(\Theta, \Theta^{t-1})$ for which MLE depends on. In the \emph{M-step}, we optimize Q w.r.t $\Theta$:
\begin{equation} \label{max-log-lik-observed-f-mm}
	\Theta^{t} = \underset{\Theta}{\operatorname{argmax}} \; Q(\Theta, \Theta^{t-1})
\end{equation}

For the case of mixture models, the expected complete data log likelihood is:
\begin{equation} \label{log-lik-expected-derivation-f-mm}
	\begin{aligned}
		Q(\Theta, \Theta^{t-1}) & \triangleq \mathbb{E} \big[\ell_{c}(\Theta) | \mathbf{X}, \Theta^{t-1}\big] \\
								& = \mathbb{E} \bigg[ \sum_{i} \log p(x_{i}, z_{i}|\theta) \bigg] \\
								& = \sum_{i} \mathbb{E} \bigg[ \log \bigg[\prod_{k} \big( \pi_{k}p(x_{i}|\theta_{k})\big)^{\mathbbm{1}(z_{i}=k)} \bigg]\bigg] \\
								& = \sum_{i} \sum_{k} \mathbb{E} \big[\mathbbm{1}(z_{i}=k)\big] \log \big[\pi_{k}p(x_{i}|\theta_{k})\big] \\
								& = \sum_{i} \sum_{k} p(z_{i}=k|x_{i},\theta_{k}^{t-1}) \log \big[\pi_{k}p(x_{i}|\theta_{k})\big] \\
								& = \sum_{i} \sum_{k} \gamma(z_{ik}) \log \big[\pi_{k}p(x_{i}|\theta_{k})\big] \\
								& = \sum_{i} \sum_{k} \gamma(z_{ik}) \log \pi_{k} + \sum_{i} \sum_{k} \gamma(z_{ik}) \log p(x_{i}|\theta_{k}) \\		
	\end{aligned}
\end{equation}
where, $\gamma(z_{ik}) \triangleq p(z_{i}=k|x_{i},\theta_{k}^{t-1})$ is the responsibility that component k takes for explaining the observation $x_{i}$, and $\mathbbm{1}(z_{i}=k)$ is the indicator function, equal to 1 if $z_{i}=k$, and 0 otherwise.

\subsection*{E-step}
The E-step can be computed using the following form for any mixture model, which is the responsibility that component k takes for explaining the observation $x_{i}$:
\begin{equation} \label{responsibilities-f-mm}
  \begin{aligned}
	\gamma(z_{ik}) & \triangleq p(z_{i}=k|x_{i},\theta_{k}^{t-1}) \\
				   & = \frac{p(z_{i}=k)p(x_{i}|z_{i}=k,\theta_{k}^{t-1})}{\sum\limits_{j=1}^{K} p(z_{i}=j)p(x_{i}|z_{i}=j,\theta_{j}^{t-1})} \\
				   & = \frac{\pi_{k}p(x_{i}|z_{i}=k,\theta_{k}^{t-1})}{\sum\limits_{j=1}^{K} \pi_{j}p(x_{i}|z_{i}=j,\theta_{j}^{t-1})}
  \end{aligned}
\end{equation}

\subsection*{M-step}
In the M-step we optimize $Q$ with respect to parameters $\Theta = (\theta_{1},...,\theta_{K},\pi_{1},...,\pi_{K})$.

For the mixing proportions $\pi_{k}$, we take the derivative of \emph{Eq. \ref{log-lik-expected-derivation-f-mm}} w.r.t. $\pi_{k}$ and set it to zero; due to the constraint that $\sum_{k=1}^{K}\pi_{k} = 1$ we introduce a Lagrange multiplier. Thus, we have:

\begin{equation} \label{derivative-mix-prop-f-mm}
  \begin{aligned}
	\frac{\partial}{\partial \pi_{k}} \bigg[  Q(\Theta, \Theta^{t-1}) + \lambda \big( \sum_{k}\pi_{k} - 1\big) \bigg] & = 0 \\
	\sum_{i} \frac{1}{\pi_{k}} \gamma(z_{ik}) + \lambda & = 0 
  \end{aligned}
\end{equation}
Setting $\lambda = - N$, the result, which is the same for any mixture model, is:
\begin{equation} \label{mixing-proportions-est-f-mm}
		\pi_{k} = \frac{1}{N} \sum_{i} \gamma(z_{ik})
\end{equation}

To derive the M-step for the parameters $\theta_{k}$, we only need to keep the terms of \emph{Eq. \ref{log-lik-expected-derivation-f-mm}} that depend on $\theta_{k}$, that is:
\begin{equation} \label{parameters-est-EM-M-f-mm}
		\ell(\theta_{k}) \triangleq \sum_{i} \sum_{k} \gamma(z_{ik}) \log p(x_{i}|\theta_{k})
\end{equation}
and we optimize them with respect to $\theta_{k}$.

Maximization of \emph{Eq. \ref{parameters-est-EM-M-f-mm}} yields different results depending on the observation model $p(x_{i}|\theta_{k})$. For most of the well known probability distributions, including the Normal, Binomial, Poisson, etc., direct maximization of $\ell(\theta_{k})$ is feasible. 

For more complex observation models, maximization of $\ell(\theta_{k})$ may be intractable, thus numerical optimization strategies \citep{Nocedal2006}, such as Conjugate Gradients (CG) algorithm \citep{Hestenes1952} should be exploited. This extension of EM algorithm is known as \emph{Generalised EM}, or GEM, and it has been proved that on each EM iteration of the GEM algorithm the log likelihood increases, and thus, converges to the maximum likelihood estimate \citep{Wu1983}.