\subsection{Bayesian Inference for FDMM} \label{fdmm-gibbs-subsect}
%\vspace*{5mm}
The full joint distribution of the FDMM model, by looking at the graphical representation factorizes as follows:
\begin{equation}
  \begin{aligned}
	p(\mathbf{X},\mathbf{Z},\mathbf{\pi},\theta;\delta,\mathcal{H}) & = p(\mathbf{X}|\mathbf{Z},\theta) p(\mathbf{Z}|\pi) p(\pi ;\delta) p(\theta ; \mathcal{H}) \\
	   & = \bigg(\prod\limits_{i=1}^{N} p(x_{i}|z_{i},\theta_{z_{i}}) p(z_{i}|\pi)\bigg) p(\pi;\delta) \bigg(\prod\limits_{k=1}^{K} p(\theta_{k}; \mathcal{H})\bigg)
  \end{aligned}
\end{equation}

where $\mathcal{H}$ is the set of all the hyper-parameters of the $\mathcal{G}_{0}$ prior distribution related to the parameters $\theta$, and $\delta$ is a K-dimensional vector with the hyper-parameters of the symmetric \emph{Dirichlet} prior. 

To perform inference in the Bayesian framework, the posterior distribution of the variables needs to be estimated. By applying Bayes' theorem and conditioning on the observed data $\mathbf{X}$, the posterior distribution is the following:
\begin{equation}%\scriptstyle
  \begin{aligned}
	p(\mathbf{Z},\mathbf{\pi},\theta|\mathbf{X} ;\delta,\mathcal{H}) & = \frac{p(\mathbf{X}|\mathbf{Z},\mathbf{\pi},\theta) p(\mathbf{Z},\mathbf{\pi},\theta ;\delta,\mathcal{H})}{p(\mathbf{X})} \\
	   & = \frac{p(\mathbf{X}|\mathbf{Z},\theta) p(\mathbf{Z}|\pi) p(\pi;\delta) p(\theta; \mathcal{H})} {p(\mathbf{X})}
  \end{aligned}
\end{equation}
where we use the fact that $\mathbf{X}$ is conditionally independent of $\pi$ given $\mathbf{Z}$, \ie $\mathbf{X} \bigCI \pi \mid \mathbf{Z}$.

This posterior is intractable to compute, thus we need to resort to an approximation scheme. The method that will be used, and the one that is extensively used for inference in mixture models, is Gibbs sampling \cite{Geman1984}. Deriving Gibbs sampling for this model requires deriving an expression for the full conditional distribution of every random variable. Since conjugate priors are used over the parameters, we can write analytically the full conditional distributions, and also we can easily draw samples from them.

By looking at the graphical model, and using the \emph{Markov blanket} for finding conditional independence between the variables, we obtain the following full conditional distributions:
\begin{equation}%\scriptstyle
  \begin{aligned}
	p(\pi|\mathbf{X},\mathbf{Z},\theta;\delta) & = p(\mathbf{\pi} | \mathbf{Z};\delta) \\ 
	p(\theta|\mathbf{X},\mathbf{Z},\pi;\mathcal{H}) & = p(\mathbf{\theta}|\mathbf{X},\mathbf{Z};\mathcal{H})  \\
	p(\mathbf{Z}|\mathbf{X},\pi, \mathbf{\theta}) & = p(\mathbf{Z}|\mathbf{X},\pi, \theta)
  \end{aligned}
\end{equation}

\noindent\textbf{Gibbs sampling algorithm}: 
\begin{itemize}
\item {Initially at simulation step 0, parameter values $\pi^{(0)}$, $\theta^{(0)}$, and $\mathbf{Z}^{(0)}$ are chosen arbitrarily.}
\item {Suppose at simulation step $\tau$ we have sampled values $\mathbf{Z}^{(\tau)}, \pi^{(\tau)}, \theta^{(\tau)}$.}
\item { At simulation step $\tau + 1$, the Gibbs sampler continues as follows:

\begin{equation*}%\scriptstyle
  \begin{aligned}
    \text{Generate} \; \; z_{i}^{(\tau+1)} \; & \propto \; \pi_{k}^{(\tau)}p(x_{i}|\theta_{k}^{(\tau)}) \; for \; k=1,...K \\
    \text{Generate} \; \; \pi_{k}^{(\tau+1)} \; & \sim \; \mathcal{D}ir\big(\delta + \sum_{i=1}^{N}\mathbbm{1}(z_{i}^{(\tau+1)}=k)\big) \\
    \text{Generate} \; \; \theta_{k}^{(\tau+1)} \; & \sim \; \mathcal{G}_{0}(\theta_{k}| \mathbf{X}, \mathbf{Z}^{(\tau+1)}=k; \mathcal{H})
  \end{aligned}
\end{equation*}
}
\end{itemize}

The choice for the prior distribution $\mathcal{G}_{0}(\cdot| \cdot; \mathcal{H})$ for the parameters $\theta$ depends on the observation model $p(\mathbf{X}|\theta)$. For example, if the observation model is a \emph{Poisson} distribution, \ie $\mathbf{X} \sim \mathcal{P}ois(\lambda)$ where $\lambda$ is the mean and variance parameter, then an appropriate conjugate prior would be a \emph{Gamma} distribution, \ie $\lambda \sim \mathcal{G}amma(\alpha, \beta)$ where $\alpha$ and $\beta$ are its hyper-parameters.