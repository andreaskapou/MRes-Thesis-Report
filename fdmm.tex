\section{Hierarchical Bayesian Mixture Models} \label{fdmm-s}

\subsection{Bayesian Statistics}
The use of Maximum Likelihood for inferring the values of the parameters $\Theta$ belongs in the \emph{frequentist} interpretation of probability. In this approach, the parameters $\Theta$ are considered to be fixed, the values are inferred by an estimator, \eg MLE, and we get error bars for the estimates by considering a distribution over datasets $\mathbf{X}$ \cite[Ch. 1]{Bishop2006}. 

On the other hand, in Bayesian statistics, probabilities provide a quantification of uncertainty or degrees of belief supported by the available evidence. In this setting, the dataset $\mathbf{X}$ is observed and hence is fixed, and we express our uncertainty in the model parameters, by considering the parameters themselves as random variables. Our initial beliefs about $\Theta$, before observing the data, are represented by a \emph{prior} probability distribution $p(\Theta)$. The effect of observing the data $\mathbf{X}$ is given through the \emph{likelihood} function $p(\mathbf{X}|\Theta)$, which is also referred to as the observation model.  The likelihood function expresses how probable are the observed data given the parameters, and is a function of the parameters. Hence, to update our beliefs about the value of $\Theta$, after observing the data $\mathbf{X}$, we use the machinery of probability theory, and more specifically Bayes' theorem, to evaluate the \emph{posterior} probability distribution:

\begin{equation}
  \begin{aligned}
	p(\Theta | \mathbf{X}) & = \frac{p(\mathbf{X}|\Theta) p(\Theta)}{p(\mathbf{X})} \\
	& = \frac{p(\mathbf{X}|\Theta) p(\Theta)}{\int p(\mathbf{X}|\Theta) p(\Theta) d\Theta}
  \end{aligned}
\end{equation}
where $p(\mathbf{X})$ is called the \emph{evidence} or \emph{marginal likelihood}. Thus, Bayes' theorem can be interpreted as follows:
\begin{equation}
	\text{posterior} = \frac{likelihood \times prior}{evidence}
\end{equation}

For completeness, below are shown the main approaches for parameter estimation.

\subsubsection*{ML Estimation of $\Theta$}
Under the Maximum Likelihood approach we seek the value of $\Theta$ that maximizes the likelihood, that is:
\begin{equation} \label{MLE-f-bayes}
	\hat{\Theta} =  \underset{\Theta}{\operatorname{argmax}} \; p(\mathbf{X}|\Theta)
\end{equation}

\subsubsection*{MAP Estimation of $\Theta$}
Under the Maximum a Posteriori approach we seek the value of $\Theta$ that maximizes the posterior $p(\Theta | \mathbf{X})$, that is:
\begin{equation} \label{MAP-f-bayes}
  \begin{aligned}
	\hat{\Theta} & =  \underset{\Theta}{\operatorname{argmax}} \; p(\Theta | \mathbf{X}) \\
	& \propto \underset{\Theta}{\operatorname{argmax}} \; p(\mathbf{X}|\Theta) p(\Theta)
  \end{aligned}
\end{equation}
where the evidence $p(\mathbf{X})$ can be ignored since it does not depend on $\Theta$. Thus, MAP estimation, in contrast to ML, incorporates in the model our prior beliefs regarding the values of the parameters.

\subsubsection*{Bayesian Estimation of $\Theta$}
Under a full Bayesian approach we compute the posterior distribution over the parameters, that is:
\begin{equation} \label{posterior-f-bayes}
  \begin{aligned}
	p(\Theta | \mathbf{X}) = \frac{p(\mathbf{X}|\Theta) p(\Theta)}{p(\mathbf{X})} 
  \end{aligned}
\end{equation}

Thus, in the Bayesian framework instead of estimating a fixed value for the parameters, we also capture the uncertainty of the estimation by computing the posterior distribution of the parameters. On the other hand, ML or MAP estimations involve in finding an optimum point estimate of the parameters, but they do not account for any uncertainty in the estimated value of the parameters. 

\subsection{Approximate Bayesian Inference}
Even though Bayesian framework is appealing, it is limited by practical difficulties since we need to marginalize over the whole parameter space, in order to compute the posterior. In most cases marginalization is computationally difficult since the integral might be intractable and difficult to approximate. To be able to compute some of the Bayesian integrals analytically, \emph{conjugate priors} should be used if possible. For a given functional form of the likelihood $p(\mathbf{X}|\Theta)$, the prior $p(\Theta)$ is said to be conjugate for that likelihood, if the posterior $p(\Theta|\mathbf{X})$ has the same functional form as the prior.

When the posterior is intractable to compute, we need to resort to approximation schemes, and these fall broadly into two main classes. Deterministic approximation schemes, such as \emph{Variational Bayes} \citep{Beal2003} or \emph{Expectation Propagation} \citep{Minka1999}, are based on picking an approximation $q(\Theta|\mathbf{X})$ from some tractable family, \eg Gaussian, and then try to make this approximation as similar as possible to the true posterior distribution $p(\Theta|\mathbf{X})$, using a cost function such as KL-divergence \cite[Ch. 21]{Murphy2012}. Thus, Variational Bayes can be seen as an extension of EM algorithm for full Bayesian parameter estimation, since it constructs a lower bound on the marginal likelihood, and attempts to optimise this bound using an iterative scheme \citep{Beal2003}.

The other approximation scheme, and the one that is used extensively in this work, is stochastic approximation of the posterior distribution using numerical sampling techniques, also known as \emph{Monte Carlo} integration \citep{Robert1999, Liu2001}. In general, Monte Carlo integration evaluates the expectation of a function $f(\mathbf{X},\Theta)$ under a probability distribution $p(\Theta)$ (\eg a posterior), by drawing $T$ samples $\Theta^{(1)},...,\Theta^{(T)}$ from $p(\Theta)$. Then, an unbiased estimate of the expectation is given by:

\begin{equation} \label{mc-f-bayes}
  \begin{aligned}
	\mathbb{E}\big[ f(\mathbf{X}, \Theta)\big] \approx \frac{1}{T} \sum\limits_{i=1}^{T} f(\mathbf{X}, \Theta^{(i)})
  \end{aligned}
\end{equation}

In most cases though, drawing samples from the probability distribution $p(\Theta)$ might be infeasible, \eg we might not be able to compute the normalisation constant. To overcome this problem a powerful framework called \emph{Markov Chain Monte Carlo} (MCMC) is extensively used \citep{Neal1998}. The main idea is that MCMC simulates draws (\ie Monte Carlo) that are slightly dependent (\ie Markov Chain), and these draws are approximately from the probability distribution of interest, i.e. the stationary distribution of the Markov Chain is the target distribution $p(\Theta)$. Then, we can use these draws to evaluate \emph{Eq. \ref{mc-f-bayes}}.

For the MCMC to converge to the target distribution of interest, some properties of then Markov Chain should hold, such as \emph{ergodicity} and \emph{irreducibility} and \emph{recurrence}. These notions will not be explained in this work, since the algorithms that will be used are almost always theoretically convergent; but the interested reader can find a detailed explanation of these notions in \citet{Robert1999} and \citet{Liu2001}

There are two main MCMC algorithms that are extensively used for Bayesian inference, \emph{Metropolis-Hastings} (MH) algorithm \citep{Metropolis1953, Hastings1970}, and \emph{Gibbs sampling} \citep{Geman1984, Gelfand1990}. MH generates a Markov Chain, where at each simulation step it proposes to move from a current state $\Theta^{t-1}$ to a new state $\Theta^{t}$ with probability $q(\Theta^{t}|\Theta^{t-1})$, where $q$ is called the \emph{proposal} or \emph{jump} distribution. The acceptance of the proposal is decided according to an appropriate criterion that will satisfy that the fraction of time spent on a specific state is proportional to the target density $p(\Theta)$.

Gibbs sampling can be seen as a special case of MH where the acceptance rate of each proposal is equal to one. The basic idea of Gibbs is that on each simulation step we sample each parameter in turn, conditioned on the values of all the remaining parameters in the distribution and the known information. This is called the \emph{full conditional} distribution, and the idea is that sampling from the full conditional will be feasible, in contrast to sampling from the full joint distribution.

For example, suppose that we have a distribution of three variables $p(\Theta|\mathbf{X}) = p(\theta_{1}, \theta_{2}, \theta_{3}|\mathbf{X})$ that we want to sample from, where $\Theta$ is the set of all variables and $\mathcal{X}$ is the known information. Suppose that at simulation step $t$ we have sampled values $\theta_{1}^{t}, \theta_{2}^{t}, \theta_{3}^{t}$. At the next simulation step, the Gibbs sampler continues as follows:

\begin{equation} \label{gibbs-f-bayes}
  \begin{aligned}
	\theta_{1}^{t+1} & \sim p(\theta_{1} | \theta_{2}^{t}, \theta_{3}^{t}, \mathbf{X}) \\
	\theta_{2}^{t+1} & \sim p(\theta_{2} | \theta_{1}^{t+1}, \theta_{3}^{t}, \mathbf{X}) \\
	\theta_{3}^{t+1} & \sim p(\theta_{3} | \theta_{1}^{t+1}, \theta_{2}^{t+1}, \mathbf{X}) 
  \end{aligned}
\end{equation}

In real applications, we may have conditional independence between random variables, and by representing the distribution as a graphical model we can infer the dependencies of the variables by looking at the \emph{Markov blanket} of each variable, which are its neighbours in the graph \cite[Ch. 24]{Murphy2012}. 

\subsection{Finite Dirichlet Mixture Models}
In a Bayesian framework, the parameters themselves are considered as random variables, thus prior distributions need to be placed over these parameters. If possible, conjugate priors should be used.

 

\begin{minipage}{0.6\textwidth}%
  \hfill
  \begin{center}
	\input{model_fdmm}
	\emph{Graphical Model of FDMM.}
  \end{center}
\end{minipage}
%\hfill
\begin{minipage}{0.1\textwidth}%\raggedright
  \begin{equation*}
  	\begin{aligned}
  		\mathbf{\pi} \; & \sim \; Dir(\mathbf{\delta}) \\
  		z_{i}|\mathbf{\pi} \; & \sim \; Cat(\mathbf{\pi}) \\
  		\theta_{k} \; & \sim \mathcal{G}_{0}; \mathcal{H} \\
  		x_{i}|z_{i}=k,\theta_{k} \; & \sim \; p(\cdot | \theta_{k})  
  	\end{aligned} 
  \end{equation*} 
\end{minipage}

\vspace*{5mm}
The full joint distribution of the FDMM model by looking at the graphical representation factorizes as follows:
\begin{equation}%\scriptstyle
	p(\mathbf{X},\mathbf{Z},\mathbf{\pi},\Theta;\delta,\mathcal{H}) = p(\mathbf{X}|\mathbf{Z},\Theta) p(\mathbf{Z}|\pi) p(\pi|\delta) p(\Theta |\mathcal{G}_{0}; \mathcal{H})
\end{equation}

where $\mathcal{H}$ is the set of all the hyper-parameters of the $\mathcal{G}_{0}$ prior distribution related to the parameters $\Theta$, and $\mathbf{\delta}$ is a K-dimensional vector with the hyper-parameters of the \emph{Dirichlet} prior. 

To do inference in the Bayesian framework, the posterior distribution of the parameters needs to be computed. By applying the Bayes Rule and conditioning on the observed data $\mathbf{X}$, the posterior distribution is simply proportional to the full joint. Thus:
 
\begin{equation}%\scriptstyle
  \begin{aligned}
	p(\mathbf{Z},\mathbf{\pi},\Theta|\mathbf{X} ;\delta,\mathcal{H}) & = \frac{p(\mathbf{X}|\mathbf{Z},\mathbf{\pi},\Theta) p(\mathbf{Z},\mathbf{\pi},\Theta ;\delta,\mathcal{H})}{p(\mathbf{X})} \\
	   & \propto p(\mathbf{X}|\mathbf{Z},\mathbf{\pi},\Theta) p(\mathbf{Z},\mathbf{\pi},\Theta ;\delta,\mathcal{H}) \\
	   & = p(\mathbf{X}|\mathbf{Z},\Theta) p(\mathbf{Z}|\pi) p(\pi|\delta) p(\Theta |\mathcal{G}_{0}; \mathcal{H}) \\
	   & = \bigg(\prod\limits_{i=1}^{N} p(x_{i}|z_{i},\theta_{z_{i}}) p(z_{i}|\pi)\bigg) p(\pi|\delta) \bigg(\prod\limits_{k=1}^{K} p(\theta_{k} |\mathcal{G}_{0}; \mathcal{H})\bigg)
  \end{aligned}
\end{equation}
where we use the fact that $\mathbf{X}$ is conditionally independent of $\pi$ given $\mathbf{Z}$, \ie $\mathbf{X} \bigCI \pi \;| \; \mathbf{Z} $. 


Deriving Gibbs sampler for this model requires deriving an expression for the conditional distribution of every random variable conditioned on all the others. More specifically, for the FDMM we have the following quantities:
\begin{equation}%\scriptstyle
	p(\mathbf{\pi}|\mathbf{X},\mathbf{Z},\Theta;\delta,\mathcal{H}) = p(\mathbf{\pi} | \mathbf{Z};\delta) \sim Dir(\delta + N_{k})
\end{equation}

\begin{equation}%\scriptstyle
	p(\mathbf{\Theta}|\mathbf{X},\mathbf{Z},\pi;\delta,\mathcal{H}) = p(\mathbf{\Theta}|\mathbf{X},\mathbf{Z};\mathcal{H}) \sim \mathcal{G}_{0}(\cdot|\mathcal{H})
\end{equation}

\begin{equation}%\scriptstyle
	p(\mathbf{Z}|\mathbf{X},\mathbf{\Theta},\pi;\delta,\mathcal{H}) = p(\mathbf{\Theta}|\mathbf{X},\mathbf{Z};\mathcal{H}) \sim \mathcal{G}_{0}(\cdot|\mathcal{H})
\end{equation}