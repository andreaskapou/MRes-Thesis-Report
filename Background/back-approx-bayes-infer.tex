\subsection{Approximate Bayesian Inference} \label{approx-bayes-infer-subsect}
Even though Bayesian framework is appealing, it is limited by practical difficulties since we need to marginalize over the whole parameter space in order to compute the posterior. In most cases marginalization is computationally difficult since the integral might be intractable and difficult to approximate. To be able to compute some of the Bayesian integrals analytically, \emph{conjugate priors} should be used if possible. For a given functional form of the likelihood $p(\mathbf{X}|\Theta)$, the prior $p(\Theta)$ is said to be conjugate for that likelihood, if the posterior $p(\Theta|\mathbf{X})$ has the same functional form as the prior.

When the posterior is intractable to compute, we need to resort to approximation schemes, and these fall broadly into two main classes. Deterministic approximation schemes, such as \emph{Variational Bayes} \citep{Beal2003} or \emph{Expectation Propagation} \citep{Minka1999}, are based on selecting an approximation $q(\Theta|\mathbf{X})$ from some tractable family, \eg Gaussian, and then trying to make this approximation as similar as possible to the true posterior distribution $p(\Theta|\mathbf{X})$, using a cost function such as KL-divergence \cite[Ch. 21]{Murphy2012}. Thus, Variational Bayes can be seen as an extension of the \emph{Expectation Maximization} (EM) algorithm for full Bayesian parameter estimation, since it constructs a lower bound on the marginal likelihood, and attempts to optimise this bound using an iterative scheme \citep{Beal2003}.

The other approximation scheme, and the one that is used extensively in this work, is stochastic approximation of the posterior distribution using numerical sampling techniques, also known as \emph{Monte Carlo} integration \citep{Robert1999, Liu2001}. In general, Monte Carlo integration evaluates the expectation of a function $f(\mathbf{X},\Theta)$ under a probability distribution $p(\Theta)$ (\eg a posterior), by drawing $T$ samples $\Theta^{(1)},...,\Theta^{(T)}$ from $p(\Theta)$. Then, an unbiased estimate of the expectation is given by:

\begin{equation} \label{mc-f-bayes}
  \begin{aligned}
	\mathbb{E}\big[ f(\mathbf{X}, \Theta)\big] \approx \frac{1}{T} \sum\limits_{i=1}^{T} f(\mathbf{X}, \Theta^{(i)})
  \end{aligned}
\end{equation}

In most cases though, drawing samples from the probability distribution $p(\Theta)$ might be infeasible, \eg we might not be able to compute the normalisation constant. To overcome this problem a powerful framework called \emph{Markov Chain Monte Carlo} (MCMC) is extensively used \citep{Neal1998}. The main idea is that MCMC simulates draws (\ie Monte Carlo) that are slightly dependent (\ie Markov Chain), and these draws are approximately from the probability distribution of interest, i.e. the stationary distribution of the Markov Chain is the target distribution $p(\Theta)$. Then, we can use these draws to evaluate \emph{Eq. \ref{mc-f-bayes}}.

For the MCMC to converge to the target distribution of interest, some properties of the Markov Chain should hold, such as \emph{ergodicity}, \emph{irreducibility}, and \emph{recurrence}. These notions will not be explained in this work, since the algorithms that will be used are almost always theoretically convergent; but the interested reader can find a detailed explanation of these notions in \citet{Robert1999} and \citet{Liu2001}

There are two main MCMC algorithms that are extensively used for Bayesian inference, \emph{Metropolis-Hastings} (MH) algorithm \citep{Metropolis1953, Hastings1970}, and \emph{Gibbs sampling} \citep{Geman1984, Gelfand1990}. 

\subsubsection*{Metropolis-Hastings algorithm}
MH generates a Markov Chain, where at each simulation step it proposes to move from a current state $\Theta^{(\tau-1)}$ to a new state $\Theta^{(\tau)}$ with probability $q(\Theta^{(\tau)}|\Theta^{(\tau -1)})$, where $q$ is called the \emph{proposal} or \emph{jump} distribution. The acceptance of the proposal is decided according to an appropriate criterion that will satisfy that the fraction of time spent on a specific state is proportional to the target density $p(\Theta)$.

\subsubsection*{Gibbs sampling algorithm}
Gibbs sampling can be seen as a special case of MH where the acceptance rate of each proposal is equal to one. The basic idea of Gibbs sampling is that on each simulation step we sample each parameter in turn, conditioned on the values of all the remaining parameters in the distribution and the known information. This is called the \emph{full conditional} distribution, and the idea is that sampling from the full conditional will be feasible, in contrast to sampling from the \emph{full joint} distribution.

For example, suppose that we have a distribution of three variables $p(\Theta|\mathbf{X}) = p(\theta_{1}, \theta_{2}, \theta_{3}|\mathbf{X})$ that we want to sample from, where $\Theta$ is the set of all variables and $\mathbf{X}$ is the known information. Suppose that at simulation step $\tau$ we have sampled values $\theta_{1}^{(\tau)}, \theta_{2}^{(\tau)}, \theta_{3}^{(\tau)}$. At the next simulation step, the Gibbs sampler continues as follows:

\begin{equation} \label{gibbs-f-bayes}
  \begin{aligned}
	\theta_{1}^{(\tau+1)} & \sim p(\theta_{1} | \theta_{2}^{(\tau)}, \theta_{3}^{(\tau)}, \mathbf{X}) \\
	\theta_{2}^{(\tau+1)} & \sim p(\theta_{2} | \theta_{1}^{(\tau+1)}, \theta_{3}^{(\tau)}, \mathbf{X}) \\
	\theta_{3}^{(\tau+1)} & \sim p(\theta_{3} | \theta_{1}^{(\tau+1)}, \theta_{2}^{(\tau+1)}, \mathbf{X}) 
  \end{aligned}
\end{equation}

In real applications, we may have conditional independence between random variables, and by representing the distribution as a graphical model we can infer the dependencies of the variables by looking at the \emph{Markov blanket} of each variable, which are its neighbours in the graph \cite[Ch. 8]{Bishop2006}. 
