\section{Finite Dirichlet Mixture Models} \label{fdmm-s}
In a fully Bayesian treatment of mixture models, the Finite Dirichlet Mixture Model (FDMM) arises \citep{Diebolt1994}. In the Bayesian paradigm, the parameters themselves are considered as random variables, thus prior distributions need to be placed over the mixing proportions $\pi$ and the parameters of the observation model $\theta$. For mathematical convenience conjugate priors should be used, if possible. Thus, a natural choice is to use a symmetric Dirichlet prior for the mixing proportions $\pi$. For the parameters $\theta$, appropriate priors should be chosen depending on the probability distribution that the observed data $\mathbf{X}$ follow.

A graphical representation of the FDMM is shown below, where $\mathcal{G}_{0}$ denoted the prior probability distribution for the parameters $\theta$. The specification of priors over the parameters, or even hyper-priors over the priors, results in the \emph{Bayesian hierarchical model}, since there are multiple layers of unknown quantities \cite{Richardson1997}. 

\begin{minipage}{0.6\textwidth}%
  \hfill
  \begin{center}
	\input{model_fdmm}
	\emph{Graphical Model of FDMM.}
  \end{center}
\end{minipage}
%\hfill
\begin{minipage}{0.1\textwidth}%\raggedright
  \begin{equation*}
  	\begin{aligned}
  		\mathbf{\pi} \; & \sim \; Dir(\delta) \\
  		z_{i}|\mathbf{\pi} \; & \sim \; Cat(\mathbf{\pi}) \\
  		\theta_{k} \; & \sim \mathcal{G}_{0}; \mathcal{H} \\
  		x_{i}|z_{i}=k,\theta_{k} \; & \sim \; p(\cdot | \theta_{k})  
  	\end{aligned} 
  \end{equation*} 
\end{minipage}

\vspace*{5mm}
The full joint distribution of the FDMM model, by looking at the graphical representation factorizes as follows:
\begin{equation}
  \begin{aligned}
	p(\mathbf{X},\mathbf{Z},\mathbf{\pi},\theta;\delta,\mathcal{H}) & = p(\mathbf{X}|\mathbf{Z},\theta) p(\mathbf{Z}|\pi) p(\pi ;\delta) p(\theta ; \mathcal{H}) \\
	   & = \bigg(\prod\limits_{i=1}^{N} p(x_{i}|z_{i},\theta_{z_{i}}) p(z_{i}|\pi)\bigg) p(\pi;\delta) \bigg(\prod\limits_{k=1}^{K} p(\theta_{k}; \mathcal{H})\bigg)
  \end{aligned}
\end{equation}

where $\mathcal{H}$ is the set of all the hyper-parameters of the $\mathcal{G}_{0}$ prior distribution related to the parameters $\theta$, and $\delta$ is a K-dimensional vector with the hyper-parameters of the symmetric \emph{Dirichlet} prior. 

To do inference in the Bayesian framework, the posterior distribution of the variables needs to be estimated. By applying the Bayes Rule and conditioning on the observed data $\mathbf{X}$, the posterior distribution is the following:
\begin{equation}%\scriptstyle
  \begin{aligned}
	p(\mathbf{Z},\mathbf{\pi},\theta|\mathbf{X} ;\delta,\mathcal{H}) & = \frac{p(\mathbf{X}|\mathbf{Z},\mathbf{\pi},\theta) p(\mathbf{Z},\mathbf{\pi},\theta ;\delta,\mathcal{H})}{p(\mathbf{X})} \\
	   & = \frac{p(\mathbf{X}|\mathbf{Z},\theta) p(\mathbf{Z}|\pi) p(\pi;\delta) p(\theta; \mathcal{H})} {p(\mathbf{X})}
  \end{aligned}
\end{equation}
where we use the fact that $\mathbf{X}$ is conditionally independent of $\pi$ given $\mathbf{Z}$, \ie $\mathbf{X} \bigCI \pi \;| \; \mathbf{Z}$. 

This posterior is intractable to compute, thus we need to resort to an approximation scheme. The method that will be used, and the one that is extensively used for inference in mixture models, is Gibbs sampling \cite{Geman1984}. Deriving Gibbs sampling for this model requires deriving an expression for the full conditional distribution of every random variable. Since conjugate priors are used over the parameters, we can write analytically the full conditional distributions, and also we can easily draw samples from them.

By looking at the graphical model, and using the \emph{Markov blanket} for finding conditional independence between the variables, we obtain the following full conditional distributions:
\begin{equation}%\scriptstyle
  \begin{aligned}
	p(\pi|\mathbf{X},\mathbf{Z},\theta;\delta) & = p(\mathbf{\pi} | \mathbf{Z};\delta) \\ 
	p(\theta|\mathbf{X},\mathbf{Z},\pi;\mathcal{H}) & = p(\mathbf{\theta}|\mathbf{X},\mathbf{Z};\mathcal{H})  \\
	p(\mathbf{Z}|\mathbf{X},\pi, \mathbf{\theta}) & = p(\mathbf{Z}|\mathbf{X},\pi, \theta)
  \end{aligned}
\end{equation}

\noindent\textbf{Gibbs sampling algorithm}: 
\begin{itemize}
\item {Initially at simulation step 0, parameter values $\pi^{(0)}$, $\theta^{(0)}$, and $\mathbf{Z}^{(0)}$ are chosen arbitrarily.}
\item {Suppose that at simulation step $\tau$ we have sampled values $(\pi^{(\tau)}, \theta^{(\tau)}, \mathbf{Z}^{(\tau)})$.}
\item { At simulation step $\tau + 1$, the Gibbs sampler continues as follows:

\begin{equation}%\scriptstyle
  \begin{aligned}
    \text{Generate} \; \; z_{i}^{(\tau+1)} \; & \propto \; \pi_{k}^{(\tau)}p(x_{i}|\theta_{k}^{(\tau)}) \; for \; k=1,...K \\
    \text{Generate} \; \; \pi_{k}^{(\tau+1)} \; & \sim \; Dir\big(\delta + \sum_{i=1}^{N}\mathbbm{1}(z_{i}^{(\tau)}=k)\big) \\
    \text{Generate} \; \; \theta_{k}^{(\tau+1)} \; & \sim \; \mathcal{G}_{0}(\theta_{k}| \mathbf{X}, \mathbf{Z}^{(\tau)}=k; \mathcal{H})
  \end{aligned}
\end{equation}
}
\end{itemize}

The choice for the prior distribution $\mathcal{G}_{0}(\cdot| \cdot; \mathcal{H})$ for the parameters $\theta$ depends on the observation model $p(\mathbf{X}|\theta)$. For example, if the observation model is a \emph{Poisson} distribution, \ie $\mathbf{X} \sim Poisson(\lambda)$ where $\lambda$ is the mean and variance parameter, then an appropriate conjugate prior would be a \emph{Gamma} distribution, \ie $\lambda \sim Gamma(\alpha, \beta)$ where $\alpha$ and $\beta$ are its hyper-parameters.