\subsection{Bayesian Inference for BCC model}\label{integr-bayes-inference-subsect}
The full joint distribution of the BCC model, by looking at the graphical representation factorizes as follows:
\begin{equation}%\scriptstyle
  \begin{aligned}
	P(\mathbb{X}, \mathbb{L}, \mathbb{C}, \pi , \Theta , \alpha) & = P(\mathbb{X}|\mathbb{L},\Theta) P(\mathbb{L}|\mathbb{C},\alpha) P(\mathbb{C}|\pi) P(\alpha) P(\pi) P(\Theta) \\
	  & = \bigg[\prod_{n}\bigg(\prod_{m} P(X_{mn}|L_{mn},\theta_{m,L_{mn}}) P(L_{mn}|C_{n},\alpha_{m})\bigg) P(C_{n}|\pi)\bigg] \\
	  & \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad P(\alpha) P(\pi) \prod_{k} P(\theta_{mk})
  \end{aligned}
\end{equation}
where the prior distribution $\mathcal{G}_{0}$ and the hyper-parameters ($\delta, \mathit{a}, \beta, \mathcal{H}$) are omitted for clarity.

To perform inference in the Bayesian framework, the posterior distribution of the variables needs to be estimated. By applying Bayes' theorem and conditioning on the observed data $\mathbb{X}$ from all data sources, the posterior distribution is the following:

\begin{equation}%\scriptstyle
	\begin{aligned}
	P(\mathbb{L},\mathbb{C},\pi,\Theta,\alpha | \mathbb{X}) & = \frac{P(\mathbb{X}|\mathbb{L},\mathbb{C},\pi,\Theta,\alpha) P(\mathbb{L},\mathbb{C},\pi,\Theta,\alpha)}{p(\mathbb{X})} \\
	& = \frac{P(\mathbb{X}|\mathbb{L},\Theta) P(\mathbb{L}|\mathbb{C},\alpha) P(\mathbb{C}|\pi) P(\alpha) P(\pi) P(\Theta)}{p(\mathbb{X})}
	\end{aligned}
\end{equation}
where we use the fact that $\mathbb{X}$ is conditionally independent of $\mathbb{C}, \pi$ and $\alpha$ given $\mathbb{L}$, \ie $\mathbb{X} \bigCI \mathbb{C}, \pi, \alpha \mid \mathbb{L}$, by looking at the \emph{Markov blanket} of $\mathbb{X}$. 

This posterior is intractable to compute, thus we need to resort to an approximation scheme. The algorithm that will be used is \emph{Gibbs sampling} \cite{Geman1984}. Deriving Gibbs sampling for this model requires deriving an expression for the \emph{full conditional} distribution of every random variable. From the graphical representation of the BCC model, we can infer the conditional independencies between the latent variables by looking at their \emph{Markov blanket}. 

For mathematical convenience, \emph{conjugate priors} should be used over the parameters, which would allow us to write analytically the full conditional distributions, and also draw samples from them. Thus:
\begin{itemize}
	\item $\pi_{k} \sim \mathcal{D}ir(\mathbf{\delta})$. For the mixing proportions a natural choice is a Dirichlet prior distribution, and the hyper-parameter vector $\mathbf{\delta}$ is set to $(1,...,1)$ so the prior is uniformly distributed in the $K-1$ simplex.
	\item $\alpha_{m} \sim \mathcal{TB}(\mathit{a}, \beta, \frac{1}{K})$. Which means that $\alpha_{m}$ follows a $Beta$ distribution with parameters $\mathit{a}, \beta$ truncated below by $\frac{1}{K}$.
	\item $\theta_{mk} \sim \mathcal{G}_{0}$. Where $\mathcal{G}_{0}$ is a conjugate prior distribution, so that sampling from the full conditional distribution $\mathcal{G}_{0}(\theta_{mk}|\mathbb{X}_{m}, \mathbb{L}_{m})$ is feasible.
\end{itemize}

\noindent\textbf{Gibbs sampling algorithm for BCC}: 
\begin{itemize}
\item {Initially at simulation step 0, initial parameter values are chosen arbitrarily.}
\item {Suppose at simulation step $\tau$ we have sampled values $\mathbb{L}^{(\tau)}, \Theta^{(\tau)}, \alpha^{(\tau)}, \mathbb{C}^{(\tau)}, \pi^{(\tau)}$.}
\item { At simulation step $\tau + 1$, the Gibbs sampler continues as follows:
\begin{equation*}%\scriptstyle
  \begin{aligned}
    L_{mn}^{(\tau+1)} \mid X_{mn}, \Theta_{m}^{(\tau)}, C_{n}^{(\tau)}, \alpha_{m}^{(\tau)} & \sim v(L_{mn}, C_{n}, \alpha_{m}) f_{m}(X_{mn}|\theta_{mk}) \\
	\theta_{mk}^{(\tau+1)} \mid \mathbb{L}_{m}^{(\tau+1)}, \mathbb{X}_{m} & \sim \mathcal{G}_{0}(\theta_{mk} \mid \mathbb{X}_{m}, \mathbb{L}_{m} = k; \mathcal{H}) \\
%	\alpha_{m}^{(\tau+1)} \mid \mathbb{L}_{m}^{(\tau+1)}, \mathbb{C}^{(\tau)} & \sim \mathcal{TB} \big(\mathit{a}+\sum_{n}\mathbbm{1}(L_{mn}=k,C_{n}=k), \\
%	  & \quad \quad \quad \; \beta + N -\sum_{n}\mathbbm{1}(L_{mn}=k,C_{n}=k), \frac{1}{K}\big) \\
	\alpha_{m}^{(\tau+1)} \mid \mathbb{L}_{m}^{(\tau+1)}, \mathbb{C}^{(\tau)} & \sim \mathcal{TB} \big(\mathit{a} + Q_{mk}, \beta + N - Q_{mk}, \frac{1}{K}\big) \\
	C_{n}^{(\tau+1)} \mid L_{mn}^{(\tau+1)}, \alpha^{(\tau+1)}, \pi^{(\tau)} & \sim \pi_{k} \prod_{m}v(L_{mn}, C_{n}, \alpha_{m}) \\
	\pi_{k}^{(\tau+1)} \mid \mathbb{C}^{(\tau+1)} & \sim \mathcal{D}ir\big(\delta + \sum_{n}\mathbbm{1}(C_{n}=k)\big)
  \end{aligned}
\end{equation*}
where $Q_{mk} = \sum_{n}\mathbbm{1}(L_{mn}=k,C_{n}=k)$, and $\mathbbm{1}(\cdot,\cdot)$ is an indicator function, equal to 1 if the quantities inside the function are satisfied, and 0 otherwise.
}
\end{itemize}
