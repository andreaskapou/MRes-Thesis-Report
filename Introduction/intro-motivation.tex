\section{Motivation} \label{motivation-intro-l}
Interpreting the genome sequence of different species is one of the major challenges in genetic and biological research. The advent of high-throughput sequencing platforms for studying and extracting genetic information from biological systems, has triggered ground-breaking discoveries and revolutionized our understanding for the \emph{genome} and \emph{epigenome} of many species. 

In particular, \emph{RNA-Seq} experiments \citep{Marioni2008} have rivaled microarrays, and now are extensively used for trancriptome profiling. \emph{Chip-Seq} \citep{Park2009} is used to quantitatively measure and analyse protein interactions with DNA, \ie histone modifications and transcription factors. \emph{RRBS} \citep{Meissner2005} uses bisulphite treatment of DNA and allows estimation of methylation level at a single-nucleotide resolution. These are only some examples of different platforms and techniques that are used to measure diverse biological components. However, despite the widespread take up of the high-throughput sequencing technology, computational data analysis is still a challenging task and crucial in order to interpret and uncover biological regulatory mechanisms \citep{Park2009}.

A \emph{model} is an abstraction of a real system and is designed to capture regularities in the observed data and subsequently make accurate predictions. Real systems, and especially biological systems, are far too complex for the modeller to design an accurate representation of the process that generated the data, and in most cases they may contain noisy and incomplete observations. Hence, simpler models are often sought which can approximate the true processes that generated the data. The \emph{machine learning} approach to constructing flexible models is to introduce a set of \emph{parameters} that specify the model and then seek a setting for those parameters that explains the data best. The idea is that if we can explain the data well by this setting of parameters, then we can also be confident when making predictions for future observations. The procedure of finding the best setting of the model parameters for the observed data is called \emph{learning} the model. A \emph{mixture model} is a widely used probabilistic model for performing exploratory analysis. It can be applied to different tasks, such as density estimation or even for predictions, but its main application is for performing \emph{clustering}, that is, identifying similar objects and grouping them together in clusters. 

Our main aim in conducting this project is to perform mixture modelling of high-throughput genomics data on two different applications. Initially, motivated by recent studies that suggest a potentially functional role for the shape of methylation profiles, we propose a novel approach for modelling and clustering DNA methylation profiles around genomic regions of interest, such as promoter regions. Assuming \emph{spatial co-dependence} of CpG sites, the proposed method fits smooth functions on the observed data using a computationally attractive approach; and subsequently mixture modelling is performed for clustering similar methylation profiles. Preliminary results on synthetic and real datasets reveal the statistical power of the method in learning the processes that generated the data, and extracting methylation patterns with clear biological importance, such as U-shape methylation profiles around \emph{transcription start sites (TSS)} that enhance transcriptional activity.

The second contribution of this project is to integrate heterogeneous types of high-throughput sequencing data using an integrative data modelling approach. We are motivated by the work of \citet{Lock2013}, who proposed an integrative statistical model termed as \emph{Bayesian Consensus Clustering} (BCC). BCC simultaneously models the dependence and the heterogeneity of the data sources, by assuming that there is a separate clustering of the objects for each data source, but these source-specific clusterings adhere loosely to an overall consensus clustering. We extend the BCC method to model data that follow different observation models, consistent with the \emph{count data} generated from high-throughput sequencing experiments.

Due to time constraint, this project was mainly concentrated on the modelling aspect by revealing that the proposed methods have the statistical power to model high-throughput sequencing data. Although initial results on real data, produced by the ENCODE project \citet{Dunham2012}, are promising for both models, comprehensive data analysis is required to assess their performance on targeted experiments with meaningful biological questions. 

%Preliminary results on synthetic and real datasets are promising statistical power 

%Recent studies have suggested that the shape of methylation profiles plays an important role in predicting gene expression, leading to a potentially functional role for methylation profiles. 
%The advent of high-throughput sequencing platforms for studying the regulation of gene expression has revolutionized our understanding for the importance of the epigenomic 'marks', such as DNA methylation and histone modifications, in cellular processes.  